---
title: "Data 621 - Homework 1"
author: " Group 4 \n Layla Quinones, Ian Castello, Dmitriy Burtsev & Esteban Aramayo "
date: "Sept. 26, 2021"
output: pdf_document
---

```{r, warning = FALSE, message = FALSE}
#libraries
library(kableExtra)
library(tidyverse)
library(tidymodels)
library(VIM)
library(naniar)
library(GGally)
library(caret)
library(psych)
library(RANN)
```


# About the Data

The data set consists of 2,276 records and 17 different variables, with each observation corresponding to a baseball teams performance in a single year. The time horizon of these data are from 1871, the same year as the first recorded professional baseball game through 2006. 

## General objective

Through linear regression, train the data to predict the number of wins.

## Challenges Right Off the Bat (so to speak)

The data set covers a very large time period. The rules and play style of baseball has changed a great deal from the late 19th-century. The season year of the team would be an important factor in improving these models. Additionally, certain clubs have bucked trends in winning or losing despite these metrics. The Boston Red Sox and Chicago Cubs had very long dry spells, even with good numbers. 

# Data Exploration

Our data is stored for easy reference among the team on GitHub. With 2,276 team observations and 17 variables. Of those, 15 are features, 1 is an index, and the remaining is our target variable for number of wins. Right away, we already know that missing values will need to be accounted for in all of this features.

```{r}
#import the data
urlTraining = "https://raw.githubusercontent.com/MsQCompSci/Data621Group4/main/HW1/moneyball-training-data.csv"
#get the data
rawData <- read.csv(urlTraining)
#Display what we imported
str(rawData)
#From this we can see that there are 2276 observations and 17 variables in total which means we have 1 variable with a unique index, 15 features and 1 target variable to train our model. There is no categorical variables and every variable is an integer. There also appears to be missing values in this data set.
```
## Summary Statistics

For each of the variables, these summary statistics provide a nice overview of each feature, its variation, and paths for potential transformations later on for model construction. Of note are the normally distributed variables, like our target variable for wins, base hits by batters, doubles by batters, walks by batters, and batters hit by pitches. The more skewed features include hits allowed, strike outs by pitchers (a very difficult thing to do consistently), and team fielding errors. Once again, we can observe the extent of the N/As and outliers that we'll have to account for. 

```{r}
#display summary statistics
summary(rawData)
#Using describe we can get even more insight into the shape of each variable
describe(rawData)
#histograms
par(mfrow = c(3,3))
for(i in 2:ncol(rawData)) {#distribution of each variable
  hist(rawData[[i]], main = colnames(rawData[i]), col = "skyblue")
}
```

### Box Plots

This visualization gives us an idea of all the outliers we have in each variable but does not give us a good sense of the distribution. We can use the visualization above to interpret shape. From the graph below we see that the variable TEAM_PITCHING_H has the greatest number of outliers which may mean we throw that variable out and not consider it in our model. We'll have to impute these outliers during preprocessing.

removed rows are NA values - This visualization gives a better sense of how many outliers there are in each variable which we may or may not have to take into account in our Data Prep for models.

There are some outliers that exceel the limit i set for this visual (3000) that we can imupute.

```{r}

rawTrainXLONG <- rawData %>%
  select(-INDEX, -TARGET_WINS) %>%
  gather(key = Variable, value = Value)
  
  
ggplot(rawTrainXLONG, aes(Variable, Value, fill = Variable)) + 
  geom_boxplot(outlier.colour="blue", 
               outlier.shape=4, 
               outlier.size=2, 
               show.legend=FALSE) + 
  ylim(0,3000) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  coord_flip()+
  labs(title="Moneyball Data Variables (ylim = 3000)")
#removed rows are NA values - This visualization gives a better sense of how many outliers there are in each variable which we may or may not have to take into account in our Data Prep for models.
#There are some outliers that exceel the limit i set for this visual (3000) that we can imupute.
#This visualization gives us an idea of all the outliers we have in each variable but does not give us a good sense of the distribution. We can use the visualization above to interpret shape. From the graph below we see that the variable TEAM_PITCHING_H has the greatest number of outliers which may mean we throw that variable out and not consider it in our model. We'll have to impute these outliers during preprocessing.
```

Before we do any transformations, we should impute missing values and outliers.

```{r}
#Lets take a look at missing values (using naniar)
gg_miss_var(rawData) +
  labs(title="Moneyball Missing Data")
#right away we can tell that TEAM_BATTING_HBP needs to be thrown out.
```

```{r}
set.seed(100)

trainRowNumbers <- createDataPartition(rawData$INDEX, p=0.8, list=FALSE)

rawTrainSplit <- rawData[trainRowNumbers,]

testData <- rawData[-trainRowNumbers,]

#seperate train x and y
rawTrainX <- rawTrainSplit %>%
  select(-INDEX, -TARGET_WINS)
rawTrainY <- rawTrainSplit %>%
  select(TARGET_WINS)
```

```{r}
#Lets see how we can throw away (as a general rule )
#What percent is missing? (must be below 60))
data.frame(TEAM_BATTING_HBP = sum(is.na(rawTrainX$TEAM_BATTING_HBP))/nrow(rawTrainX), TEAM_BASERUN_CS = sum(is.na(rawTrainX$TEAM_BASERUN_CS))/nrow(rawTrainX), TEAM_FIELDING_DP = sum(is.na(rawTrainX$TEAM_FIELDING_DP))/nrow(rawTrainX), TEAM_BASERUN_SB = sum(is.na(rawTrainX$TEAM_BASERUN_SB))/nrow(rawTrainX), TEAM_BATTING_SO = sum(is.na(rawTrainX$TEAM_BATTING_SO))/nrow(rawTrainX), TEAM_PITCHING_SO = sum(is.na(rawTrainX$TEAM_BATTING_SO))/nrow(rawTrainX))
```

Since `TEAM_BATTING_HBP` has about 91.6 percent of the data missing we will omit it. (maybe add a rationale as to why we can do this or why this is best). We will be using the histagrams for each other variable to decide how to impute during the preprocessing stage. 

```{r}
#checking for correlated vairables (looks like we have one)
#correlation matrix (use only complete observations)
corMat <- cor(rawTrainX, use = "complete.obs")
#display correlation
kable(corMat)
#correlation matrix visualization
ggcorr(rawTrainX)
```

Talk about that big red square and why we cant have correlated variables in our model. 

```{r}
#Idenitfy variablke to drop
findCorrelation(cor(rawTrainX),
                cutoff = 0.75,
                verbose = TRUE,
                names = TRUE)
#TEAM_PITCHING_HR will be omitted due to correlation (when we tested adding and subtracting this variable along with BATTING_HR it did not add any significant value)
#There are a number of correlated variables which may affect the model - we want to make sure that colinearity and correlated variables are not going to affect the model we will be using. 
```

# Data Pre-Processing

```{r}
#Drop Correlated Variable - TEAM_BATTING_HR - we may want to revisit this once we develop the model and see the changes when we add the other correlated variable - BATTING HR IS CORRELATED TO BOTH BATTING_SO AND PITCHING_HR
trainX <- rawTrainX %>%
  select(-TEAM_BATTING_HR, -TEAM_BATTING_HBP)

#Impute Missing Data based on distribution
trainX <- trainX %>%
  mutate(TEAM_BATTING_SO = ifelse(is.na(rawTrainX$TEAM_BATTING_SO), mean(rawTrainX$TEAM_BATTING_SO,na.rm=TRUE), rawTrainX$TEAM_BATTING_SO), 
         TEAM_PITCHING_SO = ifelse(is.na(rawTrainX$TEAM_PITCHING_SO), mean(rawTrainX$TEAM_PITCHING_SO,na.rm=TRUE), rawTrainX$TEAM_PITCHING_SO),
         TEAM_FIELDING_DP = ifelse(is.na(rawTrainX$TEAM_FIELDING_DP), median(rawTrainX$TEAM_FIELDING_DP,na.rm=TRUE), rawTrainX$TEAM_FIELDING_DP),
         TEAM_BASERUN_SB = ifelse(is.na(rawTrainX$TEAM_BASERUN_SB), median(rawTrainX$TEAM_BASERUN_SB,na.rm=TRUE), rawTrainX$TEAM_BASERUN_SB),
         TEAM_BASERUN_CS = ifelse(is.na(rawTrainX$TEAM_BASERUN_CS), median(rawTrainX$TEAM_BASERUN_CS,na.rm=TRUE), rawTrainX$TEAM_BASERUN_CS))
  
#MEAN IMPUTED DUE TO DISTRIBUTION: BATTING_SO, 
#MEDIAN IMPUTED DUE TO DISTRIBUTION: FIELDING_DP,BASERUN_SB, BASERUN_CS
```

# Impute outliers using knn - center and scale them

```{r}
preProcess_missingdata_model <- preProcess(trainX, method = 'knnImpute')
preProcess_missingdata_model
```

```{r}
trainX <- predict(PreProcess_trainX_model, newdata = trainX)
anyNA(trainX)
```

```{r}
dummies_model <- dummyVars(TARGET_WINS ~ ., data=trainData)
```

```{r}
#Boxplot without outliers - improved
#filtering out target and index for just predictors
trainXLONG <- trainX %>%
  gather(key = Variable, value = Value)
  
  
ggplot(trainXLONG, aes(Variable, Value, fill = Variable)) + 
  geom_boxplot(outlier.colour="blue", 
               outlier.shape=4, 
               outlier.size=2, 
               show.legend=FALSE) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  coord_flip()+
  labs(title="Moneyball Data Variables")
```

```{r}
#plot of feature as they are
      featurePlot(y = unlist(trainY), 
              x = trainX,
              plot = "scatter",
              type = c("p", "smooth"),
              span = .5,
              layout = c(3, 1))
```

# Build Model

```{r}
#Set up control to be 10-fold-crossvalidation
ctrl <- trainControl(method = "cv", number = 10) 

#ALL
modelOne <- train(x = as.data.frame(trainX), y = unlist(rawTrainY), method = "lm", trControl = ctrl)
modelOne
#When we use all the variables we do not get a satisfactory R squared value therefore based on the feature plots we began from scratch and added variables in
```

```{r}
#SOme
# R squared is better but still is bad
modelTwo <- train(x = as.data.frame(trainX %>% select (TEAM_BATTING_H: TEAM_BATTING_SO)), y = unlist(rawTrainY), method = "lm", trControl = ctrl)
modelTwo
```

```{r}
#SOme
# R squared is better but still is bad
modelThree <- train(x = as.data.frame(trainX %>% select (TEAM_BATTING_H: TEAM_PITCHING_HR)), y = unlist(rawTrainY), method = "lm", trControl = ctrl)
modelThree
```

## Lets start transforming some of these variables

```{r}
#log transformated PITCHING_BB didnt work
# minus the median didnt work

#tried adding median to BATTING3B but the r squared went down so will be leaving it alone

#Added  the median to left skew variables and now we have a highere R squared than our first model that uses all the raw variables!
fieldingE <- trainX$TEAM_FIELDING_E + median(trainX$TEAM_FIELDING_E)
baserunSB <- trainX$TEAM_BASERUN_SB + median(trainX$TEAM_BASERUN_SB)
baserunCS <- trainX$TEAM_BASERUN_CS + median(trainX$TEAM_BASERUN_CS)

trainX <- mutate(trainX, fieldingE, TEAM_BASERUN_SB = baserunSB, TEAM_BASERUN_CS = baserunCS)

#lets add it to our model
modelFour <- train(x = as.data.frame(trainX %>% select (TEAM_BATTING_H: TEAM_PITCHING_HR, TEAM_FIELDING_DP, fieldingE)), y = unlist(rawTrainY), method = "lm", trControl = ctrl)
modelFour
```

```{r}
testData2 <- predict(preProcess_missingdata_model, testData)  
```

```{r}
predict_modelOne <- predict(modelOne, testData2)

predict_modelTwo <- predict(modelTwo, testData2)
predict_modelThree <- predict(modelThree, testData2)
```

```{r}
confusionMatrix(data = predict_modelOne, reference = testData)
```
