---
title: "Data 608 HW 4 LQ"
author: "Layla Quinones"
date: "11/10/2021"
output:
  pdf_document: default
  html_document: default
---

# Libraries

```{r, message = FALSE}
library(tidyverse)
library(ggplot2)
library(VIM)
library(GGally)
library(caret)
library(broom)
library(naniar)
library(stringr)
```

# EDA

```{r}
# Load data
# Training
rawTrain <- read.csv("https://raw.githubusercontent.com/MsQCompSci/Data621Group4/main/HW4/insurance_training_data.csv", header = TRUE, stringsAsFactors = FALSE)
```


```{r}
# check to see if we need to clean the data 
glimpse(rawTrain)
```

There are 8161 observations in this data set and 26 columns. We know that `INDEX`, `TARGET_FLAG` and `TARGET_AMT` are not predictor variables. This gives us 8161 observations with 23 predictors that are a combination of int, double and character data types. We also see that the character variables will have to converted to factors in order for us to explore their distributions. Variables such and `INCOME`, `HOME_VAL`, `BLUEBOOK`, `OLDCLAIM` will be converted to numeric because they are numbers with values that have meaning in their heirarchy. 


## Missing Values 

```{r}
#plot missing values using VIM package
gg_miss_var(rawTrain)
```


There are missing variables in the columns `Car_AGE`, `AGE` and `YOJ`. None of these exceed the 10% missing data so we will continue with all variables for noe (not dropping any of them due to missing data)

## DATA CLEANING - CONVERTING DATA TYPES

```{r}
#lets remove the $ and , and put in a different variable name from numeric strings
rawTrain <- rawTrain %>% 
  mutate(INCOME = gsub("\\$", "", INCOME),     #Remove $
         HOME_VAL = gsub("\\$", "", HOME_VAL), 
         BLUEBOOK = gsub("\\$", "", BLUEBOOK), 
         OLDCLAIM = gsub("\\$", "", OLDCLAIM),
         MSTATUS = gsub("z_", "", MSTATUS),
         SEX = gsub("z_", "", SEX), 
         EDUCATION= gsub("z_", "", EDUCATION),
         JOB= gsub("z_", "", JOB),
         CAR_TYPE= gsub("z_", "", CAR_TYPE),
         URBANICITY= gsub("z_", "", URBANICITY),
         INCOME = as.numeric(gsub(",", "", INCOME)),     #remove , and cast to numeric
         HOME_VAL = as.numeric(gsub(",", "", HOME_VAL)), 
         BLUEBOOK = as.numeric(gsub(",", "", BLUEBOOK)), 
         OLDCLAIM = as.numeric(gsub(",", "", OLDCLAIM)),
         TARGET_FLAG = as.factor(TARGET_FLAG)) 

#lets also change all other character variables into factors
rawTrain[sapply(rawTrain, is.character)] <- lapply(rawTrain[sapply(rawTrain, is.character)], 
                                       as.factor)

#display summary statistics again to confirm
summary(rawTrain)
```

We get a better sense of the information available in each variable now with the data type change. 

```{r}
#histagrams for only the numerical data
histData <- rawTrain %>%
  select(AGE, HOMEKIDS, YOJ,TRAVTIME, TIF, CLM_FREQ, MVR_PTS, CAR_AGE, INCOME, HOME_VAL, BLUEBOOK, OLDCLAIM)

par(mfrow = c(3,3))
for(i in 1:ncol(histData)) {#distribution of each variable
  hist(histData[[i]], main = colnames(histData[i]), col = "red")
}
```

From the above histagrams of numerical data we can see that mose numerical variables have a right skew which may indicate that a transformation will be helpful for these variables. 

```{r}
longData <- histData %>%
  select(-HOME_VAL, -INCOME, -BLUEBOOK, -OLDCLAIM) %>%  # remove this for scale issue will plot below
  gather(key = Variable, value = Value)

# generate boxplot to identify outliers
ggplot(longData, aes(Variable, Value, fill = Variable)) +geom_boxplot(outlier.colour="blue", 
               outlier.shape=4, 
               outlier.size=2, 
               show.legend=FALSE) + 
   theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
   coord_flip()+
  labs(title="Insurance Data Variables", y="Value")
```

```{r}
longData2 <- histData %>%
  select(HOME_VAL, INCOME, BLUEBOOK, OLDCLAIM) %>%  # remove this for scale issue will plot below
  gather(key = Variable, value = Value)

# generate boxplot to identify outliers
ggplot(longData2, aes(Variable, Value, fill = Variable)) +geom_boxplot(outlier.colour="blue", 
               outlier.shape=4, 
               outlier.size=2, 
               show.legend=FALSE) + 
   theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
   coord_flip()+
  labs(title="Insurance Data Variables PART 2", y="Value")
```

From these initial box plots we can see that there are outliers specifically `TRAVTIME`, `INCOME`, `HOME_VAL` has many outliers more spread out compared to the other variables.


## Categorical Predictors - with target variable

```{r}
#plot
ggplot(rawTrain, aes(x = PARENT1, fill = TARGET_FLAG)) +
  geom_bar() +
  labs(title="Insurance Data Categorical Variables - Parent 1")

#imbalanced here
```

```{r}
ggplot(rawTrain, aes(x = MSTATUS, fill = TARGET_FLAG)) +
  geom_bar() +
  labs(title="Insurance Data Categorical Variables - Marital Status")

#less imbalanced here
```

```{r}
ggplot(rawTrain, aes(x = SEX, fill = TARGET_FLAG)) +
  geom_bar() +
  labs(title="Insurance Data Categorical Variables - SEX")

#I wouldnt consider this imbalanced but I am not sure what the threshold is for balance/imbalanced data
```

```{r}
ggplot(rawTrain, aes(x = EDUCATION, fill = TARGET_FLAG)) +
  geom_bar() +
  labs(title="Insurance Data Categorical Variables - Education")

#I wouldnt consider this imbalanced but I am not sure what the threshold is for balance/imbalanced data
```

```{r}
ggplot(rawTrain, aes(x = JOB, fill = TARGET_FLAG)) +
  geom_bar() +
  labs(title="Insurance Data Categorical Variables - Job")

#I wouldnt consider this imbalanced but I am not sure what the threshold is for balance/imbalanced data
```

```{r}
ggplot(rawTrain, aes(x = CAR_USE, fill = TARGET_FLAG)) +
  geom_bar() +
  labs(title="Insurance Data Categorical Variables - Car Use")

#Imbalanced
```

```{r}
ggplot(rawTrain, aes(x = CAR_TYPE, fill = TARGET_FLAG)) +
  geom_bar() +
  labs(title="Insurance Data Categorical Variables - Car Type")

#Imbalanced
```

```{r}
ggplot(rawTrain, aes(x = RED_CAR, fill = TARGET_FLAG)) +
  geom_bar() +
  labs(title="Insurance Data Categorical Variables - Red Car")

#Imbalanced
```

```{r}
ggplot(rawTrain, aes(x = REVOKED, fill = TARGET_FLAG)) +
  geom_bar() +
  labs(title="Insurance Data Categorical Variables - Revoked")

#Imbalanced
```

## Numeric Data - Relationship to Target

```{r}
#include target in the df for numeric data
histData <- rawTrain %>%
  select(TARGET_AMT, AGE, HOMEKIDS, YOJ,TRAVTIME, TIF, CLM_FREQ, MVR_PTS, CAR_AGE, INCOME, HOME_VAL, BLUEBOOK, OLDCLAIM)

#How do I color by Target_flag
featurePlot(x= histData[3:8], y = histData[['TARGET_AMT']])

featurePlot(x= histData[9:13], y = histData[['TARGET_AMT']])

#HOME KIDS and AGE NEED BAR CHARTS

```

## Correlation

```{r}
#correlation matrix for predictors
ggcorr(rawTrain)
```

```{r}
#Lets look at some highly correlated variables and drop them
findCorrelation(cor(histData),cutoff = 0.75, verbose = TRUE, names = TRUE)

# None of the numerical values are highly correlated
```

## Data Cleaning 

```{r, message=FALSE, }
#due to skew home_val, income  will be imputed with median
#Age YOJ with the mean

#new DF
prepTrain <- rawTrain %>%
  select(-INDEX)

#impute NAs
prepTrain$AGE[is.na(prepTrain$AGE)] <- mean(prepTrain$AGE, na.rm=TRUE)
prepTrain$YOJ[is.na(prepTrain$YOJ)] <- mean(prepTrain$YOJ, na.rm=TRUE)
prepTrain$HOME_VAL[is.na(prepTrain$HOME_VAL)] <- median(prepTrain$HOME_VAL, na.rm=TRUE)
prepTrain$INCOME[is.na(prepTrain$INCOME)] <- median(prepTrain$INCOME, na.rm=TRUE)
prepTrain$CAR_AGE[is.na(prepTrain$CAR_AGE)] <- mean(prepTrain$CAR_AGE, na.rm=TRUE)

# outlier detection and normalizing function
outlier_norm <- function(x){
  if (class(x) != "factor"){
    qntile <- quantile(x, probs=c(.25, .75))
     caps <- quantile(x, probs=c(.05, .95))
     H <- 1.5 * IQR(x, na.rm = T)
    x[x < (qntile[1] - H)] <- caps[1]
     x[x > (qntile[2] + H)] <- caps[2]
     return(x)
  }
}

#Aply the function to the columns in the dataframe
sapply(prepTrain, outlier_norm)
```


## Variable Importance

```{r}
prepTrainA <- prepTrain %>%
  select(-TARGET_AMT)

prepTrainB <- prepTrain %>%
  select(-TARGET_FLAG)
```

```{r}
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)

# train the model
modelA <- train(TARGET_FLAG~., data=prepTrainA, method="lvq", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(modelA, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
```

```{r}
# train the model
modelB <- train(TARGET_AMT~., data=prepTrainB, method="glm", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(modelB, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)

```

According to the plot above we can predict which variables would contribute best to the categorical predictions for `TARGET_FLAG`. We can use this to inform our data transformations.

## Train Test Split

```{r}
## set the seed to make your partition reproducible
set.seed(123)
trainIndex<- sort(sample(nrow(prepTrain), nrow(prepTrain)*.8))

train <- prepTrain[trainIndex, ]
test <- prepTrain[-trainIndex, ]
```

# Models
```{r}
##Baseline (logistic regression)
modelOne <- glm(formula = TARGET_FLAG ~ . - TARGET_AMT, data=train, family = "binomial" (link="logit"))
summary(modelOne)
```

What is needed next is various models to be built after transforming some of these variables based on their shape ( I would also play around with multiplying and dividing variables etc). One thing worth mentioning is that we have to predict two things. So essentially we have to come up with two types of models and test each of them. I was thinking like 3-4 models for each target showing how we are using the shape of the variables to determine transformation, feature engineering and feature selection. 

