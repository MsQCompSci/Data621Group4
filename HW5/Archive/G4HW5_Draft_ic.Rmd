---
title: "Data 621 - Homework 5"
author: "Group 4 \n Esteban Aramayo, Dmitriy Burtsev, Ian Costello, & Layla Quinones"
date: "12/6/2021"
output: pdf_document
---

```{r global-options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE,
                      fig.width = 5, fig.height = 4 )
```

# Overview

In this homework assignment, we explore, analyze and model a data set containing approximately 12,000 records representing commercially available wines. Each record has a target variable representing the number of cases purchased. Along with the target variable, there are fourteen predictor variables we will use to construct a count regression model. This model will seek to predict the number of cases that will be sold given certain properties of wine.

## Libraries Used

We use the standard libraries such as `tidyverse`, `ggplot2`, and `caret`.

```{r, message = FALSE}
# Libraries

library(tidyverse)
library(psych)
library(ggplot2)
library(VIM)
library(GGally)
library(caret)
library(broom)
library(naniar)
library(stringr)
library(ggpubr)
library(fastDummies)

```

# Data Exploration

As usual, our data are stored on GitHub at our team’s main repository for easy access across team members **(Code Appendix 1.2)**. With our initial glimpse of the data, we know that all our data set is coded correctly as either doubles or integers. **(Code Appendix 1.3)**.

```{r}
# Load data
# Training
rawTrain <- read.csv("https://raw.githubusercontent.com/MsQCompSci/Data621Group4/main/HW5/wine-training-data.csv", header = TRUE, stringsAsFactors = FALSE) %>%
  select(-"ï..INDEX")

#Testing data
rawTest <- read.csv("https://raw.githubusercontent.com/MsQCompSci/Data621Group4/main/HW5/wine-evaluation-data.csv", header = TRUE, stringsAsFactors = FALSE)%>%
  select(-"IN")
```

```{r}
# check to see if we need to clean the data 
glimpse(rawTrain)
```

## Missing Values

Looking at the missing values for the data set, `STARS` has over 3,000 missing values. This makes sense since a team of experts can't realistically rate every bottle of wine. For the others it may make sense to impute the other missing values. For the `STARS` variable we will derive one more variable `STARSRating`, indicating whether a rating was conducted (no matter the score) or not.

```{r}
#plot missing values using VIM package
gg_miss_var(rawTrain)
```

**Figure 1. Plot of Missing Values**

```{r}
rawTrain <- rawTrain %>%
  mutate(STARSRating = as.integer(ifelse(is.na(STARS),0,1)))

rawTest <- rawTest %>%
  mutate(STARSRating = as.integer(ifelse(is.na(STARS),0,1)))
```

## Summary Statistics

```{r}
describe(rawTrain)
```

**Table 1. Summary stats and description of data set**

```{r fig.width=7, fig.height=9}

histData <- rawTrain

par(mfrow = c(4,2))
for(i in 1:ncol(histData)) {#distribution of each variable
  hist(histData[[i]], main = colnames(histData[i]), col = "blue")
}
```

## Outlier Analysis

### Feature Box Plots

```{r}
# Let's identify the variables with outlier values using boxplots.

boxplots <- rawTrain %>%
  select(-TARGET, -STARSRating) %>%
  gather(Measure, Value) %>%
  ggplot(aes(y = Value)) +
  geom_boxplot() +
  facet_wrap(~Measure
             , scales = "free_y")
boxplots
```
## Numeric Data - Relationship to Target

These plot visualizations (Figure 6) **(Code Appendix 1.11)** gives us an idea of the outliers we have in each variable, but does not give us a good sense of the distribution. We can use the histograms (Figure 2) above to interpret shape. If the notches of two boxes do not overlap, then this
suggests that the medians are significantly different.

For the features we see some outliers, we can decide to either throw out that variable out altogether and not consider it in our models or impute the outliers with median values. Before deciding on a course of action, we’ll look at a few other things.

```{r }

par(mfrow = c(5,3))

#include target in the df for numeric data
histData <- rawTrain

#How do I color by Targetflag

featurePlot(x= histData[3:6], y = histData[['TARGET']])

featurePlot(x= histData[7:9], y = histData[['TARGET']])

featurePlot(x= histData[10:12], y = histData[['TARGET']])

featurePlot(x= histData[13:15], y = histData[['TARGET']])


```

## Correlation

Let's use a heat map **(Code Appendix 1.12.1)** to see the level of correlation of the numeric predictor variables. From the correlation matrix (figure 7) below and from the `findCorrelation()` function **(Code Appendix 1.12.2)**, there does not appear to be any multiple colinearity we have to account for.

```{r fig.width = 7, fig.height = 4 }
# Let's use a heat map to see the level of correlation of the numeric predictor variables.
#correlation matrix for predictors
ggcorr(rawTrain)
```

```{r}
# Let's check if there are any highly correlated variables (correlation higher than 0.75) and drop them if necessary.
findCorrelation(cor(histData),cutoff = 0.75, verbose = TRUE, names = TRUE)

# None of the numerical values are highly correlated
```

# Data Preparation

## Treatment of Missing Data


```{r, message=FALSE, results='hide'}
prepTrain <- rawTrain 

#impute NAs
prepTrain$Sulphates[is.na(prepTrain$Sulphates)] <- mean(prepTrain$Sulphates, na.rm=TRUE)
prepTrain$TotalSulfurDioxide[is.na(prepTrain$TotalSulfurDioxide)] <- mean(prepTrain$TotalSulfurDioxide, na.rm=TRUE)
prepTrain$Alcohol[is.na(prepTrain$Alcohol)] <- mean(prepTrain$Alcohol, na.rm=TRUE)
prepTrain$FreeSulfurDioxide[is.na(prepTrain$FreeSulfurDioxide)] <- mean(prepTrain$FreeSulfurDioxide, na.rm=TRUE)
prepTrain$Chlorides[is.na(prepTrain$Chlorides)] <- mean(prepTrain$Chlorides, na.rm=TRUE)
prepTrain$ResidualSugar[is.na(prepTrain$ResidualSugar)] <- mean(prepTrain$ResidualSugar, na.rm=TRUE)
prepTrain$pH[is.na(prepTrain$pH)] <- mean(prepTrain$pH, na.rm=TRUE)
```

```{r}
prepTrain <- dummy_cols(rawTrain, select_columns = "STARS", remove_selected_columns = TRUE, ignore_na = TRUE) %>%
  mutate_all(~replace(., is.na(.),0))
prepTest <- dummy_cols(rawTest, select_columns = "STARS", remove_selected_columns = TRUE, ignore_na = TRUE) %>%
  mutate_all(~replace(., is.na(.),0))

head(prepTrain)
```

```{r fig.width = 7, fig.height = 4 }
# Let's use a heat map to see the level of correlation of the numeric predictor variables.
#correlation matrix for predictors
ggcorr(prepTrain)
```

## Variable Importance

```{r eval=FALSE}
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)

# Model `modelA` was trained using the `Learning Vector Quantization` method.
modelA <- train(TARGET~., data=prepTrain, method="rf", preProcess="scale", trControl=control)
```

```{r}
importanceA <- varImp(modelA, scale=FALSE)
# summarize importance
print(importanceA, 23)
```

# Building Models 

```{r}
# set the seed to make your partition reproducible
set.seed(123)
trainIndex<- sort(sample(nrow(prepTrain), nrow(prepTrain)*.8))

train <- as.data.frame(prepTrain[trainIndex, ])
test <- as.data.frame(prepTrain[-trainIndex, ])

head(train)
```

## Model 1: Poisson

```{r}
options(scipen = 999)
poiss1 <- glm(TARGET ~ STARSRating + LabelAppeal + STARS_1 + STARS_2 + STARS_3 + STARS_4, data=train, family=poisson)
summary(poiss1)

poiss1 <- glm(TARGET ~ A, data=train, family=poisson)
summary(poiss1)
# Evaluate Model 1 with testing data set
```


